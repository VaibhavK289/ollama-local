# =============================================================================
# Allma AI Studio - Environment Configuration (Example)
# Copy to .env and customize for your deployment
# =============================================================================

# -----------------------------------------------------------------------------
# Application
# -----------------------------------------------------------------------------
APP_NAME=Allma AI Studio
APP_VERSION=1.0.0
APP_ENV=development

# -----------------------------------------------------------------------------
# Ollama Configuration
# -----------------------------------------------------------------------------
# LLM model to use (must be pulled in Ollama)
OLLAMA_MODEL=deepseek-r1:latest

# Embedding model for RAG
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

# Ollama server URL (internal Docker network)
OLLAMA_HOST=http://ollama:11434

# -----------------------------------------------------------------------------
# Backend Configuration
# -----------------------------------------------------------------------------
# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://localhost

# Database
DATABASE_URL=sqlite+aiosqlite:///./data/allma.db

# Vector store path
VECTOR_STORE_PATH=/app/data/vectorstore

# -----------------------------------------------------------------------------
# Frontend Configuration
# -----------------------------------------------------------------------------
# API URL for frontend (empty means relative to host)
VITE_API_URL=

# -----------------------------------------------------------------------------
# Production Settings (docker-compose.prod.yml)
# -----------------------------------------------------------------------------
# Domain for production
# DOMAIN=your-domain.com

# SSL/TLS
# SSL_CERT_PATH=/etc/nginx/certs/fullchain.pem
# SSL_KEY_PATH=/etc/nginx/certs/privkey.pem

# Watchtower notifications (optional)
# WATCHTOWER_NOTIFICATION_URL=

# -----------------------------------------------------------------------------
# GPU Configuration
# -----------------------------------------------------------------------------
# Set to 'cpu' to disable GPU
# OLLAMA_COMPUTE=gpu
